{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import copy\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import torchvision\n",
    "from torch.utils import model_zoo\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import skimage\n",
    "from skimage import transform as tf\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time:  2018-10-18 01:12:48.186049\n",
      "Writing Epoch 1 Triplet...\n",
      "Writing Epoch 2 Triplet...\n",
      "Writing Epoch 3 Triplet...\n",
      "Writing Epoch 4 Triplet...\n",
      "Writing Epoch 5 Triplet...\n",
      "Writing Epoch 6 Triplet...\n",
      "Writing Epoch 7 Triplet...\n",
      "Writing Epoch 8 Triplet...\n",
      "Writing Epoch 9 Triplet...\n",
      "Writing Epoch 10 Triplet...\n",
      "Writing Epoch 11 Triplet...\n",
      "Writing Epoch 12 Triplet...\n",
      "Writing Epoch 13 Triplet...\n",
      "Writing Epoch 14 Triplet...\n",
      "Writing Epoch 15 Triplet...\n",
      "Writing Epoch 16 Triplet...\n",
      "Writing Epoch 17 Triplet...\n",
      "Writing Epoch 18 Triplet...\n",
      "Writing Epoch 19 Triplet...\n",
      "Writing Epoch 20 Triplet...\n",
      "Writing Epoch 21 Triplet...\n",
      "Writing Epoch 22 Triplet...\n",
      "Writing Epoch 23 Triplet...\n",
      "Writing Epoch 24 Triplet...\n",
      "Writing Epoch 25 Triplet...\n",
      "Writing Epoch 26 Triplet...\n",
      "Writing Epoch 27 Triplet...\n",
      "Writing Epoch 28 Triplet...\n",
      "Writing Epoch 29 Triplet...\n",
      "Writing Epoch 30 Triplet...\n",
      "Cost time 0:28:36.327094\n"
     ]
    }
   ],
   "source": [
    "# Sample triplets and save paths as texts (No use)\n",
    "# Cost about 30 minutes to sample this textfile\n",
    "import os\n",
    "\n",
    "train_dir = \"tiny-imagenet-200/train\"\n",
    "imtp_name = os.listdir(train_dir)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print(\"Start Time: \", start_time)\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    print(\"Writing Epoch {} Triplet...\".format(n+1))\n",
    "    filename = \"ImagePath/Epoch_{}_Triplet.txt\".format(n+1)\n",
    "    file1 = open(filename,\"w\") \n",
    "    for imtpidx in range(num_tp):\n",
    "        typename = imtp_name[imtpidx]\n",
    "        imfolder = \"/\".join([train_dir, typename, \"images\"])\n",
    "        #imfolder = os.path.join(train_dir, typename, \"images\")\n",
    "        im_name = os.listdir(imfolder)\n",
    "        for imidx in range(num_im):\n",
    "            im_qr = \"/\".join([\"HW5\",imfolder, im_name[imidx]])\n",
    "            #im_qr = os.path.join(imfolder, im_name[imidx]) # Save paths as text\n",
    "\n",
    "            samp_im_ps = np.random.choice(im_name[:imidx]+im_name[imidx+1:])\n",
    "            im_ps = \"/\".join([\"HW5\",imfolder, samp_im_ps])\n",
    "            #im_ps = os.path.join(imfolder, samp_im_ps)\n",
    "\n",
    "            imtp_ng = np.random.choice(imtp_name[:imtpidx]+imtp_name[imtpidx+1:]) # Type of trainset\n",
    "            imfolder_ng = \"/\".join([train_dir, imtp_ng, \"images\"])\n",
    "            #imfolder_ng = os.path.join(train_dir, imtp_ng, \"images\")\n",
    "            im_ng_name = os.listdir(imfolder_ng)\n",
    "            samp_im_ng = np.random.choice(im_ng_name)\n",
    "            im_ng = \"/\".join([\"HW5\",imfolder_ng, samp_im_ng])\n",
    "            #im_ng = os.path.join(imfolder_ng, samp_im_ng)\n",
    "\n",
    "            file1.writelines(\"\\t\".join([im_qr, im_ps, im_ng, typename, \"\\n\"]))\n",
    "    file1.close()\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Cost time\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create validation set divided with labels (No use)\n",
    "import os\n",
    "import shutil\n",
    "# Load validation labels\n",
    "file2 = open(\"tiny-imagenet-200/val/val_annotations.txt\", \"r\")\n",
    "valpic = []\n",
    "vallabel = []\n",
    "tt = file2.readlines()\n",
    "for line in tt:\n",
    "    lines = re.split(\"\\t\",line)\n",
    "    valpic.append(lines[0])\n",
    "    vallabel.append(lines[1])\n",
    "file2.close()\n",
    "# Create directories and move pictures\n",
    "valfolder = \"tiny-imagenet-200/val/images/\"\n",
    "alllabel = list(set(vallabel))\n",
    "for label in alllabel:\n",
    "    os.makedirs(valfolder+label)\n",
    "for (pic,label) in zip(valpic,vallabel):\n",
    "    shutil.move(valfolder+pic, valfolder+label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In validation set, transform all grey images to \"RGB\", all image sizes to (224 x 224 x 3), and save (No Use)\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "valfolder = \"tiny-imagenet-200/val/images/\"\n",
    "trans_rz = transforms.Resize(size=(224,224))\n",
    "\n",
    "valall = os.listdir(valfolder)\n",
    "for i, vfold in enumerate(valall):\n",
    "    valcls = os.listdir(os.path.join(valfolder,vfold))\n",
    "    for image in valcls:\n",
    "        picpath = os.path.join(valfolder, vfold, image)\n",
    "        pic = Image.open(picpath)\n",
    "        if pic.mode == \"L\":\n",
    "            pic = pic.convert(\"RGB\")\n",
    "        pic = trans_rz(pic)\n",
    "        pic.save(picpath,\"JPEG\") # Then save the new pic with the same name\n",
    "    if (i+1)%20==0:\n",
    "        print(\"{} classes of pictures have been transformed...\".format(i+1))\n",
    "        now_time = datetime.datetime.now()\n",
    "        print(\"Cost time: {}\".format(now_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similarly for training set, but will take 10 times longer than validation set (No use)\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "trainfolder = \"tiny-imagenet-200/train\"\n",
    "trainall = os.listdir(trainfolder)[170:]\n",
    "trans_rz = transforms.Resize(size=(224,224))\n",
    "\n",
    "for j, tfold in enumerate(trainall):\n",
    "    traincls = os.listdir(os.path.join(trainfolder, tfold, \"Images\"))\n",
    "    for image in traincls:\n",
    "        picpath = os.path.join(trainfolder, tfold, \"Images\", image)\n",
    "        pic = Image.open(picpath)\n",
    "        if pic.mode == \"L\":\n",
    "            pic = pic.convert(\"RGB\")\n",
    "        pic = trans_rz(pic)\n",
    "        pic.save(picpath, \"JPEG\") # Then save the new pic with the same name\n",
    "    if (j+1)%10==0:\n",
    "        print(\"{} classes of pictures have been transformed...\".format(j+1))\n",
    "        now_time = datetime.datetime.now()\n",
    "        print(\"Cost time: {}\".format(now_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_tp = 200 # Number of classes\n",
    "num_im = 500 # Number of images in each class\n",
    "total_im = num_tp * num_im\n",
    "num_epochs = 30\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "embedding_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load validation set\n",
    "valfolder = \"tiny-imagenet-200/val/images/\"\n",
    "valdataset = torchvision.datasets.ImageFolder(valfolder, transform=transforms.ToTensor()) \n",
    "val_loader = Data.DataLoader(valdataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load training set for convenience of embedding\n",
    "trainfolder = \"tiny-imagenet-200/train\"\n",
    "train_embed_dataset = torchvision.datasets.ImageFolder(trainfolder, transform=transforms.ToTensor()) \n",
    "train_embed_loader = Data.DataLoader(train_embed_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "# Problem: How to substitute the last linear layer so that embedding vector instead of classification is returned\n",
    "# Define pre-trained model ResNet18\n",
    "def resnet18(pretrained=True):\n",
    "    model = torchvision.models.resnet.ResNet(torchvision.models.resnet.BasicBlock,[2,2,2,2])\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet18-5c106cde.pth',model_dir=\"./\"))\n",
    "    return model\n",
    "\n",
    "mod = resnet18()\n",
    "model = copy.deepcopy(mod)\n",
    "model.fc = nn.Linear(512, embedding_size, bias=True) # Adapt for our model\n",
    "model = model.to(device)\n",
    "\n",
    "# Build criterion and optimizer\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TinyImageDataset(Data.Dataset):\n",
    "    def __init__(self, impath, label=None, train=True):\n",
    "        self.path = impath\n",
    "        self.label = label\n",
    "        self.train = train\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        index (int): Index of Dataset\n",
    "        \n",
    "        Return:\n",
    "        if train: \n",
    "            img_triplet (np.array, 3 x 224 x 224 x 3)\n",
    "        else: (Will return error if label==None and train==False)\n",
    "            img (np.array, 224 x 224 x 3), label (str) \n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            img_triplet_path = re.split(string=self.path[index], pattern=\"\\t\")\n",
    "            img_triplet_path = [re.sub(\"HW5/\",\"\",img_triplet_path[i]) for i in range(3)]\n",
    "            img_triplet = [np.reshape(io.imread(img_triplet_path[i]),newshape=(3,224,224)) for i in range(3)]\n",
    "            return img_triplet\n",
    "        else:\n",
    "            img = io.imread(self.path[index])\n",
    "            label = self.label[index]\n",
    "            return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Triplet_Generate_TinyImageNet200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In each epoch, zip a new trainset and train the network\n",
    "for epoch in range(num_epochs):\n",
    "    # Load new dataset\n",
    "    file1 = open(\"ImagePath\\\\Epoch_{}_Triplet.txt\".format(epoch+1), \"r\")\n",
    "    tm = file1.readlines()\n",
    "    train_dataset = TinyImageDataset(impath = tm)\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Run model\n",
    "    total = 0\n",
    "    correct = 0 # Count accuracy in each epoch\n",
    "    loss_total = 0 # Total loss in each epoch\n",
    "    for i, image_triplet in enumerate(train_loader):\n",
    "        image_qr = image_triplet[0].to(device)\n",
    "        image_ps = image_triplet[1].to(device)\n",
    "        image_ng = image_triplet[2].to(device)\n",
    "            \n",
    "        # Forward\n",
    "        embed_qr = model(image_qr)\n",
    "        embed_ps = model(image_ps)\n",
    "        embed_ng = model(image_ng)\n",
    "        loss = criterion(embed_qr, embed_ps, embed_ng)\n",
    "        total += image_qr.size(0)\n",
    "        correct += (loss==0).sum().item()\n",
    "        loss_total += loss.item()\n",
    "            \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Epoch[{}/{}], Total Loss {:4f}, Accuracy {:4f}%\".format(\n",
    "            epoch+1, num_epochs, loss_total, correct/total*100))\n",
    "    \n",
    "    # Compute test accuracy on validation set\n",
    "    if (epoch+1)%5 == 0: # Do testing on validation set\n",
    "        train_embed = np.empty([total_im, embedding_size])\n",
    "        # First, compute training embeddings\n",
    "        for i, (image,label) in enumerate(train_embed_loader):\n",
    "            image_embed = model(image)\n",
    "            train_embed[i,:] = image_embed\n",
    "        # Then, for each validation image\n",
    "        count_correct = 0\n",
    "        totalcount = len(val_loader)\n",
    "        for j, (image,label) in enumerate(val_loader):\n",
    "            image_embed = model(image)\n",
    "            image_exp_embed = np.tile(image_embed, reps=(total_im,1))\n",
    "            \n",
    "            dist_embed = train_embed - image_exp_embed\n",
    "            dist = np.sum(dist_embed**2, axis=1, keepdims=True)\n",
    "            argdistidx = np.argsort(dist)[:30]\n",
    "            argdistlabel = argdistidx // num_im\n",
    "            arglabel = max(set(argdistlabel), key=argdistlabel.count)\n",
    "            if arglabel == label:\n",
    "                count_correct += 1\n",
    "        print(\"Validation accuracy in {} epoch: {}%\".format(epoch+1, count_correct/totalcount * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In each epoch, zip a new trainset and train the network\n",
    "# This version has been abandoned. I have chosen to preprocess the images instead.\n",
    "for epoch in num_epochs:\n",
    "    file1 = open(\"ImagePath\\\\Epoch_{}_Triplet.txt\".format(epoch+1), \"r\")\n",
    "    tm = file1.readlines()\n",
    "    im_qr_arr = np.zeros([1,224,224,3])\n",
    "    im_ps_arr = np.zeros([1,224,224,3])\n",
    "    im_ng_arr = np.zeros([1,224,224,3])\n",
    "    # Zip image arrays as Tensors\n",
    "    for idx in range(num_tp*num_im):\n",
    "        tmp_triplet = tm[idx]\n",
    "        tmp_path = re.split(string=tmp_triplet, pattern=\"\\t\")\n",
    "        im_qr = io.imread(tmp_path[0])\n",
    "        im_qr = grey2rgb(im_qr)\n",
    "        im_ps = io.imread(tmp_path[1])\n",
    "        im_ps = grey2rgb(im_ps)\n",
    "        im_ng = io.imread(tmp_path[2])\n",
    "        im_ng = grey2rgb(im_ng)\n",
    "        # Do transforms on these images\n",
    "        # Website: http://scikit-image.org/docs/dev/auto_examples/transform/plot_rescale.html\n",
    "        im_qr = tf.resize(im_qr,output_shape=(224,224))\n",
    "        im_ps = tf.resize(im_ps,output_shape=(224,224))\n",
    "        im_ng = tf.resize(im_ng,output_shape=(224,224))\n",
    "        # Concat each image to corresponding Tensors\n",
    "        im_qr_arr = np.concatenate((im_qr_arr, np.expand_dims(im_qr, axis=0)))\n",
    "        im_ps_arr = np.concatenate((im_ps_arr, np.expand_dims(im_ps, axis=0)))\n",
    "        im_ng_arr = np.concatenate((im_ng_arr, np.expand_dims(im_ng, axis=0)))\n",
    "    tm.close()\n",
    "    # Zip as Tensor\n",
    "    im_qr_arr = im_qr_arr[1:,:,:,:]\n",
    "    im_ps_arr = im_ps_arr[1:,:,:,:]\n",
    "    im_ng_arr = im_ng_arr[1:,:,:,:]\n",
    "    im_qr_tensor = torch.Tensor(im_qr_arr)\n",
    "    im_ps_tensor = torch.Tensor(im_ps_arr)\n",
    "    im_ng_tensor = torch.Tensor(im_ng_arr)\n",
    "    # Zip images in a Data.Dataloader\n",
    "    trainset = Data.TensorDataset(im_qr_tensor, im_ps_tensor, im_ng_tensor)\n",
    "    train_loader = Data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Run model\n",
    "    total = 0\n",
    "    correct = 0 # Count accuracy in each epoch\n",
    "    loss_total = 0 # \n",
    "    for i,(image_qr, image_ps, image_ng) in enumerate(train_loader):\n",
    "        image_qr = image_qr.to(device)\n",
    "        image_ps = image_ps.to(device)\n",
    "        image_ng = image_ng.to(device)\n",
    "            \n",
    "        # Forward\n",
    "        embed_qr = model(image_qr)\n",
    "        embed_ps = model(image_ps)\n",
    "        embed_ng = model(image_ng)\n",
    "        loss = criterion(embed_qr, embed_ps, embed_ng)\n",
    "        total += image_qr.size(0)\n",
    "        correct += (loss==0).sum().item()\n",
    "        loss_total += loss.item()\n",
    "            \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Epoch[{}/{}], Total Loss {:4f}, Accuracy {:4f}%\".format(\n",
    "            epoch+1, num_epochs, loss_total, correct/total*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
