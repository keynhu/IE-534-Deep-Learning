{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"language_rnn_checkpoint.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Epoch': 75,\n",
       " 'Test_acc': [0.1921580000000001,\n",
       "  0.21266360000000015,\n",
       "  0.22426240000000003,\n",
       "  0.2313943999999998,\n",
       "  0.2365840000000001,\n",
       "  0.24104359999999997,\n",
       "  0.24453000000000008,\n",
       "  0.24721039999999997,\n",
       "  0.24905639999999998,\n",
       "  0.25074120000000005,\n",
       "  0.2524464,\n",
       "  0.2540400000000001,\n",
       "  0.2551092,\n",
       "  0.25573520000000005,\n",
       "  0.2568604,\n",
       "  0.2578596000000001,\n",
       "  0.2586648000000001,\n",
       "  0.2594952,\n",
       "  0.26005880000000015,\n",
       "  0.2606432000000001,\n",
       "  0.26106039999999997,\n",
       "  0.2611347999999999,\n",
       "  0.2616292,\n",
       "  0.2624212000000001,\n",
       "  0.26322280000000003,\n",
       "  0.2638548,\n",
       "  0.26331400000000005,\n",
       "  0.264058,\n",
       "  0.2641699999999999,\n",
       "  0.2645088000000001,\n",
       "  0.2646455999999999,\n",
       "  0.26498160000000004,\n",
       "  0.2658479999999999,\n",
       "  0.26528320000000005,\n",
       "  0.26557119999999995,\n",
       "  0.2660075999999999,\n",
       "  0.26650439999999986,\n",
       "  0.2665235999999999,\n",
       "  0.26631960000000005,\n",
       "  0.26671279999999986,\n",
       "  0.26678320000000016,\n",
       "  0.2675772,\n",
       "  0.2666783999999999,\n",
       "  0.26702919999999986,\n",
       "  0.2674476000000001,\n",
       "  0.26729640000000005,\n",
       "  0.26734320000000006,\n",
       "  0.26749160000000016,\n",
       "  0.2676691999999999,\n",
       "  0.26744640000000003,\n",
       "  0.26935,\n",
       "  0.26909520000000003,\n",
       "  0.26953360000000004,\n",
       "  0.2697224000000001,\n",
       "  0.269442,\n",
       "  0.2696967999999999,\n",
       "  0.26992279999999985,\n",
       "  0.2702692,\n",
       "  0.2698291999999998,\n",
       "  0.2702307999999999,\n",
       "  0.2703212000000001,\n",
       "  0.27027719999999994,\n",
       "  0.27025960000000004,\n",
       "  0.2701787999999999,\n",
       "  0.2698596000000001,\n",
       "  0.2701708000000001,\n",
       "  0.27035120000000007,\n",
       "  0.2700015999999999,\n",
       "  0.27054200000000006,\n",
       "  0.2707312,\n",
       "  0.2703040000000001,\n",
       "  0.2703507999999999,\n",
       "  0.27034920000000007,\n",
       "  0.27086040000000006,\n",
       "  0.2706667999999999],\n",
       " 'Train_acc': [0.12398800000000001,\n",
       "  0.16841120000000004,\n",
       "  0.18364799999999992,\n",
       "  0.19357360000000012,\n",
       "  0.2000976,\n",
       "  0.20484399999999992,\n",
       "  0.20905360000000003,\n",
       "  0.2117728,\n",
       "  0.21451359999999997,\n",
       "  0.21692799999999982,\n",
       "  0.21927999999999997,\n",
       "  0.2215344000000001,\n",
       "  0.2226608,\n",
       "  0.22391840000000018,\n",
       "  0.22562319999999988,\n",
       "  0.22661439999999988,\n",
       "  0.2273752,\n",
       "  0.22951280000000007,\n",
       "  0.2299640000000001,\n",
       "  0.2308528,\n",
       "  0.23215439999999982,\n",
       "  0.23264160000000006,\n",
       "  0.23393919999999996,\n",
       "  0.23429999999999995,\n",
       "  0.2348776000000001,\n",
       "  0.23617600000000002,\n",
       "  0.23715440000000002,\n",
       "  0.2372823999999999,\n",
       "  0.23836320000000003,\n",
       "  0.23871680000000012,\n",
       "  0.23947440000000003,\n",
       "  0.2398312,\n",
       "  0.240344,\n",
       "  0.24036160000000004,\n",
       "  0.24155600000000002,\n",
       "  0.24255919999999986,\n",
       "  0.24283999999999997,\n",
       "  0.2426344,\n",
       "  0.24340960000000006,\n",
       "  0.2444872,\n",
       "  0.24472000000000022,\n",
       "  0.2454248000000001,\n",
       "  0.2463215999999999,\n",
       "  0.24585360000000003,\n",
       "  0.24669439999999998,\n",
       "  0.24690880000000012,\n",
       "  0.2481104000000001,\n",
       "  0.24747840000000015,\n",
       "  0.24849680000000007,\n",
       "  0.24870240000000005,\n",
       "  0.2513168,\n",
       "  0.2525368,\n",
       "  0.25324479999999994,\n",
       "  0.2530032000000001,\n",
       "  0.2539872000000001,\n",
       "  0.25440559999999995,\n",
       "  0.2546944000000001,\n",
       "  0.25446480000000016,\n",
       "  0.2551424,\n",
       "  0.25467999999999996,\n",
       "  0.25543040000000006,\n",
       "  0.2552736000000002,\n",
       "  0.2548335999999999,\n",
       "  0.25606880000000004,\n",
       "  0.2558600000000001,\n",
       "  0.25639359999999983,\n",
       "  0.25605199999999984,\n",
       "  0.25613759999999997,\n",
       "  0.25691919999999985,\n",
       "  0.25638960000000005,\n",
       "  0.2568560000000001,\n",
       "  0.25706399999999985,\n",
       "  0.2569983999999999,\n",
       "  0.25709360000000003,\n",
       "  0.25716320000000004],\n",
       " 'Train_loss': [6.317909530639648,\n",
       "  5.006321769714355,\n",
       "  4.796429443359375,\n",
       "  4.67445027923584,\n",
       "  4.588563045501709,\n",
       "  4.533116180419922,\n",
       "  4.479638984680176,\n",
       "  4.440660221099853,\n",
       "  4.404972038269043,\n",
       "  4.378812953948975,\n",
       "  4.348907211303711,\n",
       "  4.324787120819092,\n",
       "  4.302999145507813,\n",
       "  4.289454780578613,\n",
       "  4.270786441802978,\n",
       "  4.25475745010376,\n",
       "  4.241966808319092,\n",
       "  4.2236744346618655,\n",
       "  4.213250282287597,\n",
       "  4.202148929595947,\n",
       "  4.190343502044677,\n",
       "  4.17786445236206,\n",
       "  4.1680844764709475,\n",
       "  4.161532173156738,\n",
       "  4.149320533752442,\n",
       "  4.138860080718994,\n",
       "  4.127597484588623,\n",
       "  4.127183631896973,\n",
       "  4.113990356445313,\n",
       "  4.108289459228516,\n",
       "  4.100943937301635,\n",
       "  4.097829694747925,\n",
       "  4.087370573043823,\n",
       "  4.083591621398925,\n",
       "  4.0783070297241215,\n",
       "  4.070031076431275,\n",
       "  4.06210552406311,\n",
       "  4.0597296276092525,\n",
       "  4.052776702880859,\n",
       "  4.045540748596191,\n",
       "  4.041961555480957,\n",
       "  4.033342443466187,\n",
       "  4.030059970855713,\n",
       "  4.030098278045655,\n",
       "  4.019885738372802,\n",
       "  4.015944067001342,\n",
       "  4.006102205276489,\n",
       "  4.008592964172363,\n",
       "  3.999178939819336,\n",
       "  4.000402101516723,\n",
       "  3.974085994720459,\n",
       "  3.9623382015228272,\n",
       "  3.9587700061798095,\n",
       "  3.952278434753418,\n",
       "  3.951552730560303,\n",
       "  3.9476912899017336,\n",
       "  3.941225431442261,\n",
       "  3.9422438201904297,\n",
       "  3.940394245147705,\n",
       "  3.938790294647217,\n",
       "  3.93282546043396,\n",
       "  3.9345923862457277,\n",
       "  3.9380606174468995,\n",
       "  3.9292658805847167,\n",
       "  3.9269152126312257,\n",
       "  3.927033281326294,\n",
       "  3.9290629863739013,\n",
       "  3.922929138183594,\n",
       "  3.9189447174072267,\n",
       "  3.9261551570892332,\n",
       "  3.922216833114624,\n",
       "  3.9207862758636476,\n",
       "  3.9191208629608156,\n",
       "  3.916363645553589,\n",
       "  3.914648235321045]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(80,5)\n",
    "x = torch.LongTensor(np.random.randn(500,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500, 80]), torch.Size([39500]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x[:,1:].contiguous().view(-1)\n",
    "x.size(), y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index out of range at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1532505617613\\work\\aten\\src\\th\\generic/THTensorMath.cpp:352",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-26f804fdf082>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    108\u001b[0m         return F.embedding(\n\u001b[0;32m    109\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index out of range at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1532505617613\\work\\aten\\src\\th\\generic/THTensorMath.cpp:352"
     ]
    }
   ],
   "source": [
    "embed = embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6024, 0.6548, 0.6765, 0.6843, 0.686 , 0.689 , 0.6882, 0.6898,\n",
       "       0.69  , 0.6888, 0.6887, 0.689 , 0.689 , 0.6889, 0.689 , 0.6891,\n",
       "       0.6888, 0.689 , 0.6891, 0.6891])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = os.listdir(\"Part 2 Output\")\n",
    "param_group = []\n",
    "cpt = torch.load(os.path.join(\"Part 2 Output\",checkpoint[15]))\n",
    "#np.around(cpt[\"Train_loss\"],decimals=6), np.around(cpt[\"Train_acc\"],3)\n",
    "np.around(cpt[\"Test_acc\"],decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8154, 0.8729, 0.8789, 0.8927, 0.902 , 0.9109, 0.9117, 0.9125,\n",
       "       0.9155, 0.9177, 0.9175, 0.9161, 0.9171, 0.9205, 0.9158, 0.9215,\n",
       "       0.9233, 0.922 , 0.9214, 0.9216])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = os.listdir(\"Part 1 Output\")\n",
    "param_group = []\n",
    "cpt = torch.load(os.path.join(\"Part 1 Output\",checkpoint[1]))\n",
    "#np.around(cpt[\"Train_loss\"],decimals=6), np.around(cpt[\"Train_acc\"],3)\n",
    "np.around(cpt[\"Test_acc\"],decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_train = [[\"it\", \"is\", \"raining\", \"now\", \".\"], [\"i\", \"am\", \"taking\", \"class\", \".\"],\n",
    "           [\"how\", \"is\", \"it\", \"going\", \"?\"], [\"do\", \"you\", \"like\", \"food\", \"?\"]]\n",
    "token_ids = np.asarray([[token for token in x] for x in x_train])\n",
    "token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am', 'how', '.', '?', 'is', 'taking', 'like', 'class', 'you',\n",
       "       'the', 'going', 'a', 'it', 'raining', 'do', 'i', 'food'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## word_to_id and id_to_word. associate an id to every unique token in the training data\n",
    "## Hence we can build our own vocabulary\n",
    "all_tokens = itertools.chain.from_iterable(x_train)\n",
    "word_to_id = {token:idx for idx,token in enumerate(set(all_tokens))}\n",
    "\n",
    "all_tokens = itertools.chain.from_iterable(x_train)\n",
    "id_to_word = [token for idx, token in enumerate(set(all_tokens))]\n",
    "id_to_word = np.asarray(id_to_word)\n",
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a2410cd200d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_to_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0midx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mid_to_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mid_to_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_to_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mid_to_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "word_to_id = {token:idx for idx,token in enumerate(set(x_train))}\n",
    "\n",
    "id_to_word = [token for idx, token in enumerate(set(x_train))]\n",
    "id_to_word = np.asarray(id_to_word)\n",
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['is', 'it', 'am', 'like', '.', 'how', 'taking', 'class', 'i',\n",
       "        'going', 'the', 'a', 'raining', 'do', '?', 'you', 'food'],\n",
       "       dtype='<U7'),\n",
       " array([2., 2., 2., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's sort the indices by word frequency instead of random\n",
    "x_train_token_ids = [[word_to_id[token] for token in x] for x in x_train]\n",
    "count = np.zeros(id_to_word.shape)\n",
    "for x in x_train_token_ids:\n",
    "    for token in x:\n",
    "        count[token] += 1\n",
    "indices = np.argsort(-count)\n",
    "id_to_word = id_to_word[indices]\n",
    "count = count[indices]\n",
    "id_to_word, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## recreate word_to_id based on sorted list\n",
    "word_to_id = {token: idx for idx, token in enumerate(id_to_word)}\n",
    "\n",
    "## assign -1 if token doesn't appear in our dictionary\n",
    "## add +1 to all token ids, we went to reserve id=0 for an unknown token\n",
    "x_train_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_train]\n",
    "#x_test_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 13, 5],\n",
       " [9, 3, 7, 12, 8, 5],\n",
       " [6, 1, 2, 10, 15],\n",
       " [14, 16, 4, 11, 17, 15]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save dictionary\n",
    "np.save('exp_dictionary.npy',np.asarray(id_to_word))\n",
    "\n",
    "## save training data to single text file\n",
    "with io.open('exp_imdb_train.txt','w',encoding='utf-8') as f:\n",
    "    for tokens in x_train_token_ids:\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get all of the training reviews (including unlabeled reviews)\n",
    "train_directory = '/projects/training/bauh/NLP/aclImdb/train/'\n",
    "\n",
    "pos_filenames = os.listdir(train_directory + 'pos/')\n",
    "neg_filenames = os.listdir(train_directory + 'neg/')\n",
    "unsup_filenames = os.listdir(train_directory + 'unsup/')\n",
    "\n",
    "pos_filenames = [train_directory+'pos/'+filename for filename in pos_filenames]\n",
    "neg_filenames = [train_directory+'neg/'+filename for filename in neg_filenames]\n",
    "unsup_filenames = [train_directory+'unsup/'+filename for filename in unsup_filenames]\n",
    "\n",
    "filenames = pos_filenames + neg_filenames + unsup_filenames\n",
    "\n",
    "x_train = []\n",
    "for filename in filenames:\n",
    "    with io.open(filename,'r',encoding='utf-8') as f:\n",
    "        line = f.readlines()[0]\n",
    "    line = line.replace('<br />',' ')\n",
    "    line = line.replace('\\x96',' ')\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = [w.lower() for w in line]\n",
    "    x_train.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get all of the test reviews\n",
    "test_directory = '/projects/training/bauh/NLP/aclImdb/test/'\n",
    "\n",
    "pos_filenames = os.listdir(test_directory + 'pos/')\n",
    "neg_filenames = os.listdir(test_directory + 'neg/')\n",
    "\n",
    "pos_filenames = [test_directory+'pos/'+filename for filename in pos_filenames]\n",
    "neg_filenames = [test_directory+'neg/'+filename for filename in neg_filenames]\n",
    "\n",
    "filenames = pos_filenames+neg_filenames\n",
    "\n",
    "x_test = []\n",
    "for filename in filenames:\n",
    "    with io.open(filename,'r',encoding='utf-8') as f:\n",
    "        line = f.readlines()[0]\n",
    "    line = line.replace('<br />',' ')\n",
    "    line = line.replace('\\x96',' ')\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = [w.lower() for w in line]\n",
    "    x_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## word_to_id and id_to_word. associate an id to every unique token in the training data\n",
    "## Hence we can build our own vocabulary\n",
    "all_tokens = itertools.chain.from_iterable(x_train)\n",
    "id_to_word = [token for idx, token in enumerate(set(all_tokens))]\n",
    "id_to_word = np.asarray(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## let's sort the indices by word frequency instead of random\n",
    "x_train_token_ids = [[word_to_id[token] for token in x] for x in x_train]\n",
    "count = np.zeros(id_to_word.shape)\n",
    "for x in x_train_token_ids:\n",
    "    for token in x:\n",
    "        count[token] += 1\n",
    "indices = np.argsort(-count)\n",
    "id_to_word = id_to_word[indices]\n",
    "count = count[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## recreate word_to_id based on sorted list\n",
    "word_to_id = {token: idx for idx, token in enumerate(id_to_word)}\n",
    "\n",
    "## assign -1 if token doesn't appear in our dictionary\n",
    "## add +1 to all token ids, we went to reserve id=0 for an unknown token\n",
    "x_train_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_train]\n",
    "x_test_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
