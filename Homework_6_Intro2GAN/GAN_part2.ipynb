{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook basically refers to https://courses.engr.illinois.edu/ie534/fa2018/secure/GAN.html#part-1-training-a-gan-on-cifar10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GD_ckpt = torch.load(\"GD_checkpoint.pth.tar\")\n",
    "test_acc = GD_ckpt[\"Test_Accuracy\"]\n",
    "train_acc = GD_ckpt[\"Train Accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8672, 147)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(test_acc), np.argmax(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94122, 195)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_acc), np.argmax(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GD_ckpt = torch.load(\"GD_checkpoint.pth.tar\")\n",
    "test_acc = GD_ckpt[\"Test_Accuracy\"]\n",
    "train_acc = GD_ckpt[\"Train Accuracy\"]\n",
    "\n",
    "plt.plot(train_acc, color='blue',linestyle = '-', label=\"Train\")\n",
    "plt.plot(test_acc, color=\"red\", linestyle = '-', label=\"Test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train Accuracy of Generator-Discriminator\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Gen_Disc_Accuracy_Plot.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save function\n",
    "def save_checkpoint(obj, is_best, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(obj, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, \"best_\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot images\n",
    "def custom_plot(samples):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    gs = gridspec.GridSpec(10, 10)\n",
    "    gs.update(wspace=0.02, hspace=0.02)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis(\"off\")\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        plt.imshow(sample)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.7, 1.0), ratio=(1.0,1.0)),\n",
    "    transforms.ColorJitter(\n",
    "            brightness=0.1*torch.randn(1),\n",
    "            contrast=0.1*torch.randn(1),\n",
    "            saturation=0.1*torch.randn(1),\n",
    "            hue=0.1*torch.randn(1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BatchSize = 128\n",
    "num_classes = 10\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                                        transform=transform_train)\n",
    "train_loader = Data.DataLoader(trainset, batch_size=BatchSize, shuffle=True, num_workers=8)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "                                       transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=BatchSize, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Generator model with structure given in reference\n",
    "LongGenConv = nn.Sequential(nn.Linear(in_features=100, out_features=196*4*4, bias=True),\n",
    "                         nn.ConvTranspose2d(in_channels=196,out_channels=196,kernel_size=4,stride=2,padding=1),\n",
    "                         nn.BatchNorm2d(num_features=196),\n",
    "                         nn.ReLU(inplace=True), # Conv layer 1\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=1),\n",
    "                         nn.BatchNorm2d(num_features=196),\n",
    "                         nn.ReLU(inplace=True), # Conv layer 2\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=1), \n",
    "                         nn.BatchNorm2d(num_features=196),\n",
    "                         nn.ReLU(inplace=True), # Conv layer 3\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=1),\n",
    "                         nn.BatchNorm2d(num_features=196),\n",
    "                         nn.ReLU(inplace=True), # Conv layer 4\n",
    "                         nn.ConvTranspose2d(in_channels=196,out_channels=196,kernel_size=4,stride=2,padding=1),\n",
    "                         nn.BatchNorm2d(num_features=196),\n",
    "                         nn.ReLU(inplace=True), # Conv layer 5\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=1),\n",
    "                         nn.BatchNorm2d(num_features=196),\n",
    "                         nn.ReLU(inplace=True), # Conv layer 6\n",
    "                         nn.ConvTranspose2d(in_channels=196,out_channels=196,kernel_size=4,stride=2,padding=1),\n",
    "                         nn.BatchNorm2d(num_features=196),\n",
    "                         nn.ReLU(inplace=True), # Conv layer 7\n",
    "                         nn.Conv2d(in_channels=196,out_channels=3,kernel_size=3,stride=1,padding=1)) # Conv layer 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Discriminator model with structure given in reference\n",
    "LongConv = nn.Sequential(nn.Conv2d(in_channels=3,out_channels=196,kernel_size=3,stride=1,padding=1),\n",
    "                         nn.LayerNorm(normalized_shape=(196,32,32)),\n",
    "                         nn.LeakyReLU(inplace=True), # Conv layer 1\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=2),\n",
    "                         nn.LayerNorm(normalized_shape=(196,16,16)),\n",
    "                         nn.LeakyReLU(inplace=True), # Conv layer 2\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=1), \n",
    "                         nn.LayerNorm(normalized_shape=(196,16,16)),\n",
    "                         nn.LeakyReLU(inplace=True), # Conv layer 3\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=2),\n",
    "                         nn.LayerNorm(normalized_shape=(196,8,8)),\n",
    "                         nn.LeakyReLU(inplace=True), # Conv layer 4\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=1),\n",
    "                         nn.LayerNorm(normalized_shape=(196,8,8)),\n",
    "                         nn.LeakyReLU(inplace=True), # Conv layer 5\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=1),\n",
    "                         nn.LayerNorm(normalized_shape=(196,8,8)),\n",
    "                         nn.LeakyReLU(inplace=True), # Conv layer 6\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=1),\n",
    "                         nn.LayerNorm(normalized_shape=(196,8,8)),\n",
    "                         nn.LeakyReLU(inplace=True), # Conv layer 7\n",
    "                         nn.Conv2d(in_channels=196,out_channels=196,kernel_size=3,stride=1,padding=2),\n",
    "                         nn.LayerNorm(normalized_shape=(196,4,4)),\n",
    "                         nn.LeakyReLU(inplace=True), # Conv layer 8\n",
    "                         nn.MaxPool2d(kernel_size=4,stride=4)) # Max Pooling                       \n",
    "Scorer = nn.Linear(in_features=196,out_features=1,bias=True)\n",
    "Classifier = nn.Linear(in_features=196,out_features=10,bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a Generator class\n",
    "class GenConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenConvNet, self).__init__()\n",
    "        self.GenLayer = LongGenConv\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.GenLayer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a Discriminator class\n",
    "class DiscConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscConvNet, self).__init__()\n",
    "        self.ConvLayer = LongConv\n",
    "        self.Class = Classifier\n",
    "        self.Score = Scorer\n",
    "        \n",
    "    def forward(self,x):\n",
    "        ConvOut = self.ConvLayer(x)\n",
    "        Out = ConvOut.reshape(ConvOut.shape[0],-1)\n",
    "        ScoreOut = self.Score(Out)\n",
    "        ClassOut = self.Class(Out)\n",
    "        return ScoreOut, ClassOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "genmodel = GenConvNet().to(device)\n",
    "discmodel = DiscConvNet().to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_g = torch.optim.Adam(genmodel.parameters(), lr=learning_rate, betas=(0,0.9))\n",
    "optimizer_d = torch.optim.Adam(discmodel.parameters(), lr=learning_rate, betas=(0,0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a way to compute gradient\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    DIM = 32\n",
    "    LAMBDA = 10\n",
    "    alpha = torch.rand(BatchSize, 1)\n",
    "    alpha = alpha.expand(BatchSize, int(real_data.nelement()/batch_size)).contiguous()\n",
    "    alpha = alpha.view(BatchSize, 3, DIM, DIM)\n",
    "    alpha = alpha.to(device)\n",
    "    \n",
    "    fake_data = fake_data.view(batch_size, 3, DIM, DIM)\n",
    "    interpolates = alpha * real_data.detach() + ((1 - alpha) * fake_data.detach())\n",
    "\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates.requires_grad = True\n",
    "\n",
    "    disc_interpolates, _ = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)                              \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training process of Generator\n",
    "for param in discmodel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "genmodel.zero_grad()\n",
    "\n",
    "# Generate the input noise\n",
    "rand_label = np.random.randint(0, num_classes, BatchSize)\n",
    "noise = np.random.normal(0,1,(BatchSize,100))\n",
    "rand_label_onehot = np.zeros((BatchSize,num_classes))\n",
    "rand_label_onehot[np.arange(BatchSize), rand_label] = 1\n",
    "noise[np.arange(BatchSize), :num_classes] = rand_label_onehot[np.arange(BatchSize)]\n",
    "noise = noise.astype(np.float32)\n",
    "ts_noise = torch.from_numpy(noise).to(device) # Create noise as a Tensor\n",
    "fake_label = torch.from_numpy(rand_label).to(device) # Create fake label as a Tensor\n",
    "\n",
    "# Generate fake images, evaluate it by discriminator\n",
    "fake_data = genmodel(ts_noise)\n",
    "gen_score, gen_class = discmodel(fake_data)\n",
    "gen_loss = criterion(gen_class, fake_label)\n",
    "gen_cost = -gen_score.mean() + gen_loss\n",
    "gen_cost.backward()\n",
    "\n",
    "optimizer_g.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training process of Discriminator\n",
    "for param in discmodel.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "discmodel.zero_grad()\n",
    "\n",
    "# Generate the input noise\n",
    "rand_label = np.random.randint(0, num_classes, BatchSize)\n",
    "noise = np.random.normal(0,1,(BatchSize,100))\n",
    "rand_label_onehot = np.zeros((BatchSize,num_classes))\n",
    "rand_label_onehot[np.arange(BatchSize), rand_label] = 1\n",
    "noise[np.arange(BatchSize), :num_classes] = rand_label_onehot[np.arange(BatchSize)]\n",
    "noise = noise.astype(np.float32)\n",
    "ts_noise = torch.from_numpy(noise).to(device) # Create noise as a Tensor\n",
    "fake_label = torch.from_numpy(rand_label).to(device) # Create fake label as a Tensor\n",
    "\n",
    "# Generate fake images and evaluate\n",
    "with torch.no_grad():\n",
    "    fake_data = genmodel(ts_noise)\n",
    "disc_fake_score, disc_fake_class = discmodel(fake_data)\n",
    "disc_fake_loss = criterion(disc_fake_class, fake_label)\n",
    "\n",
    "# Train discriminator with input from the discriminator\n",
    "real_data = images.to(device)\n",
    "read_label = labels.to(device).long()\n",
    "disc_real_score, disc_real_class = discmodel(real_data)\n",
    "disc_real_loss = criterion(disc_real_class, real_label)\n",
    "\n",
    "prediction = disc_real_class.data.max(1)[1]\n",
    "accuracy = float(prediction.eq(real_label.data).sum()) / float(BatchSize) * 100\n",
    "grad_penalty = calc_gradient_penalty(discmodel, real_data, fake_data)\n",
    "\n",
    "disc_cost = disc_fake_score.mean() - disc_real_score.mean() + disc_real_loss + disc_fake_loss + grad_penalty\n",
    "disc_cost.backward()\n",
    "\n",
    "optimizer_d.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "discmodel.eval()\n",
    "with torch.no_grad():\n",
    "    test_acc = []\n",
    "    for idx, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, output = discmodel(images)\n",
    "        \n",
    "        prediction = output.data.max(1)[1]\n",
    "        accuracy = float(prediction.eq(labels).sum()) / float(BatchSize) * 100\n",
    "        test_acc.append(accuracy)\n",
    "        accuracy_test = np.mean(test_accu)\n",
    "now_time = datetime.datetime.now()\n",
    "print(\"Testing\", accuracy_test, now_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = custom_plot(samples)\n",
    "plt.savefig(\"output_%s.png\" % str(epoch).zfill(3), bbox_inches=\"tight\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot fake images\n",
    "with torch.no_grad():\n",
    "    genmodel.eval()\n",
    "    samples = genmodel(ts_noise).cpu().numpy()\n",
    "    samples += 1\n",
    "    samples /= 2\n",
    "    samples = samples.transpose(0,2,3,1)\n",
    "    genmodel.train()\n",
    "    \n",
    "fig = custom_plot(samples)\n",
    "plt.savefig(\"output_%s.png\" % str(epoch).zfill(3), bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "torch.save(genmodel, \"tempGenModel.ckpt\")\n",
    "torch.save(discmodel, \"tempDiscModel.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete process of training\n",
    "start_time = datetime.datetime.now()\n",
    "train_acc_seq = []\n",
    "test_acc_seq = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # this avoids overflow\n",
    "    if epoch > 5:\n",
    "        for group in optimizer_g.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = optimizer.state[p]\n",
    "                if('step' in state and state['step']>=1024):\n",
    "                    state['step'] = 1000\n",
    "        for group in optimizer_d.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = optimizer.state[p]\n",
    "                if('step' in state and state['step']>=1024):\n",
    "                    state['step'] = 1000\n",
    "    genmodel = genmodel.train()\n",
    "    discmodel = discmodel.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # train the generator\n",
    "        for param in discmodel.parameters():\n",
    "            param.requires_grad = False\n",
    "        genmodel.zero_grad()\n",
    "        ### Generate the input noise\n",
    "        rand_label = np.random.randint(0, num_classes, BatchSize)\n",
    "        noise = np.random.normal(0,1,(BatchSize,100))\n",
    "        rand_label_onehot = np.zeros((BatchSize,num_classes))\n",
    "        rand_label_onehot[np.arange(BatchSize), rand_label] = 1\n",
    "        noise[np.arange(BatchSize), :num_classes] = rand_label_onehot[np.arange(BatchSize)]\n",
    "        noise = noise.astype(np.float32)\n",
    "        ts_noise = torch.from_numpy(noise).to(device) # Create noise as a Tensor\n",
    "        fake_label = torch.from_numpy(rand_label).to(device) # Create fake label as a Tensor\n",
    "        ### Generate fake images, evaluate it by discriminator\n",
    "        fake_data = genmodel(ts_noise)\n",
    "        gen_score, gen_class = discmodel(fake_data)\n",
    "        gen_loss = criterion(gen_class, fake_label)\n",
    "        gen_cost = -gen_score.mean() + gen_loss\n",
    "        gen_cost.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # train the discriminator with input from generator\n",
    "        for param in discmodel.parameters():\n",
    "            param.requires_grad = True\n",
    "        discmodel.zero_grad()\n",
    "        ### Generate fake images and evaluate\n",
    "        with torch.no_grad():\n",
    "            fake_data = genmodel(ts_noise)\n",
    "        disc_fake_score, disc_fake_class = discmodel(fake_data)\n",
    "        disc_fake_loss = criterion(disc_fake_class, fake_label)\n",
    "        ### Train discriminator\n",
    "        real_data = images.to(device)\n",
    "        read_label = labels.to(device).long()\n",
    "        disc_real_score, disc_real_class = discmodel(real_data)\n",
    "        disc_real_loss = criterion(disc_real_class, real_label)\n",
    "\n",
    "        prediction = disc_real_class.data.max(1)[1]\n",
    "        accuracy = float(prediction.eq(real_label.data).sum()) / float(BatchSize) * 100\n",
    "        grad_penalty = calc_gradient_penalty(discmodel, real_data, fake_data)\n",
    "\n",
    "        disc_cost = disc_fake_score.mean() - disc_real_score.mean() + disc_real_loss + disc_fake_loss + grad_penalty\n",
    "        disc_cost.backward()\n",
    "\n",
    "        optimizer_d.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the input noise\n",
    "label = np.asarray(list(range(10))*10)\n",
    "noise = np.random.normal(0,1,(100,100))\n",
    "label_onehot = np.zeros((100,10))\n",
    "label_onehot[np.arange(100), label] = 1\n",
    "noise[np.arange(100), :10] = label_onehot[np.arange(100)]\n",
    "noise = noise.astype(np.float32)\n",
    "\n",
    "ts_noise = torch.from_numpy(noise).to(device) # Create noise as a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import types\n",
    "types.MethodType"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
