\documentclass[12pt]{article}
\usepackage{amsmath}

\title{IE 534 Homework 2 Report}
\author{Hanwen Hu}

\begin{document}
In this homework, I have achieved a training accuracy of 98.88\%, and testing accuracy of 97.34\%. A brief introduction of my implementation is given below.\\

\section{Basic Design}
Suppose the input image $X\in \mathbf{R}^{d\times d}$, and output $Y\in \{0,1\}^{P}$ which has been coded with one-hot, i.e. all elements of Y are 0, except only one element as 1. The convolution kernel size is set to be $(k\times k)$, and there are $C$ channels in total,i.e. $K\in \mathbf{R}^{k\times k\times C}$. Hence the weight and bias of the full-connected layer are given as $W\in \mathbf{R}^{P\times (d-k+1)\times (d-k+1)\times C}$ and $b\in \mathbf{R}^{P}$. The activation function of hidden layer is ReLu, and that of output layer is Softmax.\\

\section{Forward Propagation}
Let $*$ represent convolution, and $\cdotp$ as direct dot product. The process of forward propagation is given by
\begin{equation}
\begin{aligned}
  Z &= X * K\quad &\in \mathbf{R}^{(d-k+1)\times (d-k+1)\times C},\\
H &= \sigma_{ReLu}(Z)\quad &\in \mathbf{R}^{(d-k+1)\times (d-k+1)\times C},\\
U_p &= W_{p,:,:,:}\cdotp Z + b_p\quad &\in \mathbf{R},\\
\hat{Y} &= \sigma_{Softmax}(U)\quad &\in [0,1]^P.
\end{aligned}
\end{equation}

\section{Backward Propagation}
Suppose the loss function between $Y$ and $\hat{Y}$ is given by $\rho(Y,\hat{Y})=-Y^T log(\hat{Y})$, then the process of backward propagation is given by
\begin{equation}
\begin{aligned}
\frac{\partial \rho}{\partial U} &= Y - \hat{Y},\\
\frac{\partial \rho}{\partial b} &= \frac{\partial \rho}{\partial U},\\
\frac{\partial \rho}{\partial W_{p,:,:,:}} &= \frac{\partial \rho}{\partial U_p}H,\\
\frac{\partial \rho}{\partial H_{i,j,c}} &= \frac{\partial \rho}{\partial U}\cdotp W_{:,i,j,c},\\
\frac{\partial \rho}{\partial Z} &= \mathbf{1}_{Z>0} \odot \frac{\partial \rho}{\partial H},\\
\frac{\partial \rho}{\partial K_c} &= X * \frac{\partial \rho}{\partial Z_{:,:,c}}.
\end{aligned}
\end{equation}

During the implementation, the "numpy.tensordot" function is adopted to avoid "for" loops in the update of $\frac{\partial \rho}{\partial W}$ and $\frac{\partial \rho}{\partial H}$.

\section{Update Process}
Similarly with the neural network, the update process is presented below.
\begin{equation}
\begin{aligned}
b &= b - \alpha \frac{\partial \rho}{\partial b},\\
W &= W - \alpha \frac{\partial \rho}{\partial W},\\
K &= K - \alpha \frac{\partial \rho}{\partial K}.
\end{aligned}
\end{equation}

\section{Hyperparameter Settings and Time Cost}
This CNN has set the kernel size to be (3 $\times$ 3), number of channels as 5, number of epochs as 10, and original learning rate as 0.01, which decays to 10\% for every 5 epochs.\\


\end{document}